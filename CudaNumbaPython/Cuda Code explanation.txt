In the cuda code, we use @cuda.jit to tell python to give this to cuda/gpu. This means all code inside here must work with our numba package.
So, we can no longer just use random.random(), as we need to find a thread safe random number generator that numba can use. So the one we use is
xoroshiro128p_uniform_float32
we can now generate random numbers on our gpu, which is the bulk of the montecarlo simulation.

When the iterations are 10,000, the amount of simulated pairs (x,y) is actually 64*24*10000 = 15,360,000

So we compare out output times between 15,000,000 in serial, and the 10,000 iteration run for our cuda parallel python code.

Serial took 7.188s, and the cuda code takes 0.9582506s  This is much much faster. And I checked on much higher iteration run sizes (1,000,000) which would imply 
1,000,000*64*24 = 1,536,000,000 and this was still only taking around 1-2s

This is on my RTX 2080ti, and I could see when I did that gpu utilization rose to 100%.